{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c112ee5",
   "metadata": {},
   "source": [
    "# Intuition Of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c99b45",
   "metadata": {},
   "source": [
    "#### Decoding the Multi-Head Attention:-\n",
    "\n",
    "* <a href=\"#attention-layer-components\">Attention Layer And Components</a>\n",
    "* <a href=\"#steps-multi-head-attention\"> Steps To Compute Multihead Attention </a>\n",
    "* <a href=\"#implementation\"> Implementing Multihead Attention Module </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88eab3",
   "metadata": {},
   "source": [
    "## Attention Layer and components\n",
    "<a name=\"attention-layer-components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f256e",
   "metadata": {},
   "source": [
    "![Attention layer](https://miro.medium.com/max/2000/1*9nUzdaTbKzJrAsq1qqJNNA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cff59",
   "metadata": {},
   "source": [
    "####  Three linear layer:\n",
    "* A linear layer is a type of neural network that does not include an activation function. Its primary functions are to map input data onto output data and to change the dimensionality of the input data through dimensionality reduction. The weights of the linear layer can be updated during downstream tasks.\n",
    "\n",
    "\n",
    "* Transformers are a type of neural network architecture that use three linear layers.\n",
    "\n",
    "\n",
    "* The Attention layer in a Transformer model takes input in the form of three parameters: Query, Key, and Value. Each of these parameters represents each word in the input sequence as a vector, with similar structure.\n",
    "\n",
    "\n",
    "* The input to each of the linear layers in the Transformer model includes positional embeddings, which are obtained earlier in the model and each layer has its own weights.\n",
    "\n",
    "\n",
    "* The output from the Query and Key parameters is used to derive similarity between the input tokens. This similarity is then used to weight the values, producing the final output from the Attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c622b",
   "metadata": {},
   "source": [
    "* #### Lets look at the initution of how Transformers uses similarity to predict the relevent words based on Query and Keys:\n",
    "    \n",
    "    we can find the similarity of two vectors, considering a cosine similarity as follows,\n",
    "    \n",
    "    $$\\text{cosine similarity} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\lVert \\mathbf{b} \\rVert} $$\n",
    "\n",
    "    so, similary finding the similarity between two matrices as follows:\n",
    "\n",
    "    $$\\text{similarity(a,b)} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}^T}{\\lVert \\sqrt {scaling} \\rVert} $$\n",
    "\n",
    "    now, deducing the above to find the similarities between Query and Keys:\n",
    "\n",
    "    $$\\text{similarity(Q,K)} = \\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\lVert \\sqrt {d} \\rVert} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdcf9d",
   "metadata": {},
   "source": [
    "* The dot product between the query and key is first computed which is termed as an attention filter.\n",
    " initially the scores in the attention filter are random values but post the training the scores hold a better meaning.\n",
    " \n",
    "\n",
    "* Then, the attention scores are scaled by dividing the scores with the dimension of the key vector (as per author of paper Attention is all you need). this is done to minimize the variance of the dot product: $$ Q*K^T $$\n",
    "\n",
    "\n",
    "* Using the softmax function . the scores are transformed between 0 and 1.\n",
    "\n",
    "* the tranformed attention scores is then multiplied with the value matrix to obtain the final attention filter which inturn is termed as an *Attention Head*.\n",
    "\n",
    "\n",
    "$$\\text{Attention(Q,K,V)} = softmax(\\frac{\\mathbf{Q} \\cdot \\mathbf{K}^T}{\\lVert \\sqrt {d} \\rVert}) * V $$\n",
    "\n",
    "\n",
    "* The Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. \n",
    "\n",
    "\n",
    "* The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word.(Note, as per the paper \"Attention is all you need\" 8 such attention head are emphasised).\n",
    "\n",
    "\n",
    "\n",
    "* multi-head attention mechanism linearly projects the queries, keys, and values multiple times, using a different learned projection each time.\n",
    "\n",
    "\n",
    "\n",
    "* The single attention mechanism is then applied to each of these projections in parallel to produce  outputs, which, in turn are concatenated and projected again over a linear layer to produce a final result.\n",
    "\n",
    " \n",
    " \n",
    "* The idea behind multi-head attention is to allow the attention function to extract information from different representation subspaces, which would otherwise be impossible with a single attention head.\n",
    "\n",
    "\n",
    "$$\\text{Multihead(Q,K,V)} = Concat(head1,head2.....,headh) * W^0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5012fa",
   "metadata": {},
   "source": [
    "## Steps To Compute Multihead Attention:\n",
    "<a name=\"steps-multi-head-attention\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c949493",
   "metadata": {},
   "source": [
    "\n",
    "* The step-by-step procedure for computing multi-head attention is as follows:\n",
    "\n",
    "* Compute the linearly projected versions of the queries, keys, and values through multiplication with the respective weight matrices $$ W^Q , W^k and W^V $$  $$ one for each Head^i $$.\n",
    "\n",
    "* Apply the single attention function for each head by\n",
    "\n",
    "    1. multiplying the queries and keys matrices \n",
    "    \n",
    "    2. applying the scaling and softmax operations, and \n",
    "    \n",
    "    3. weighting the values matrix to generate an output for each head.\n",
    "    \n",
    "\n",
    "* Concatenate the outputs of the heads. $$ Head^i ,i=1,2....h$$\n",
    "\n",
    "* Apply a linear projection to the concatenated output through multiplication with the weight matrix, to generate the final result of multi-head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1752ea",
   "metadata": {},
   "source": [
    "## Implementing Multihead Attention Module:\n",
    "<a name=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "558ba271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_dim = kwargs.get(\"input_dim\")\n",
    "        self.d_model = kwargs.get(\"d_model\")\n",
    "        self.num_heads = kwargs.get(\"num_heads\")\n",
    "        self.head_dim = self.d_model // self.num_heads\n",
    "        ## creating 3 linear layer to represent (Query,Key and Value)\n",
    "        self.qkv_layer = nn.Linear(self.input_dim , 3 * self.d_model)\n",
    "        ## final layer for projecting the concatinated attention filter obatined from all the multiple heads\n",
    "        self.linear_layer = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "        ## initializing the three 3 linear vectors\n",
    "        self.query = []\n",
    "        self.key= []\n",
    "        self.value=[]\n",
    "        \n",
    "        ## flag to check if its encoder or decoder. \n",
    "        self.is_decoder = kwargs.get(\"is_decoder\")\n",
    "        \n",
    "    def scaled_dot_product(self):\n",
    "        d_k = self.query.size()[-1]\n",
    "        scaled = torch.matmul(self.query, self.key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Masking is done only for decoders\n",
    "        if self.is_decoder:\n",
    "            # creating the mask vector.\n",
    "            mask = torch.full(scaled.size() , float('-inf'))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # masking it with scaled vectors\n",
    "            scaled += mask\n",
    "        \n",
    "        self_attention = Func.softmax(scaled, dim=-1)\n",
    "        values = torch.matmul(self_attention, self.value)\n",
    "        return values, self_attention\n",
    "    \n",
    "    def compute_attention(self, input_vector):\n",
    "        batch_size, sequence_length, input_dim = input_vector.size()\n",
    "        qkv = self.qkv_layer(input_vector)\n",
    "        \n",
    "        ## creating 8 (query,key,values) for all the 3 attention heads\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        self.query, self.key,self.value = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        ## obtaining the attention filters and attention scores\n",
    "        values, attention = self.scaled_dot_product()\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        ## projecting over a liner layer to obtain the final multihead attention values\n",
    "        output_vector = self.linear_layer(values)\n",
    "        return output_vector\n",
    "    \n",
    "    def get_attention_details(self,output_vector):\n",
    "        print(f\"The Query,Key,Value layer is ----> {self.qkv_layer} \\n\")\n",
    "        print(f\"The Query vector is ----> {self.query.size()} \\n\")\n",
    "        print(f\"The key vector is ----> {self.key.size()} \\n\")\n",
    "        print(f\"The value vector is ----> {self.value.size()} \\n\")\n",
    "        print(f\"The multihead output vector is ----> {output_vector.size()} \\n\")\n",
    "        print(f\"Multi-head Attention is at decoder ----> {self.is_decoder} \\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae449bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "\n",
    "INPUT_DIM = 1024    ## Dimension of the input\n",
    "MODEL_DIM = 512     ## Dimention of the vectors in multihead attention\n",
    "NUM_HEADS = 8       ## No.of attention head required in multihead attention\n",
    "SEQ_LEN = 5         ## Sequence length of input\n",
    "BATCH_SIZE = 30     ## Batch size\n",
    "\n",
    "## computing random Input Vector\n",
    "input_vector = torch.randn( (BATCH_SIZE, SEQ_LEN, INPUT_DIM) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8b81385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if computing the multihead attention at Decoder, use attribute. is_decoder = True\n",
    "\n",
    "multi_head = MultiheadAttention(input_dim=INPUT_DIM, d_model=MODEL_DIM, num_heads=NUM_HEADS,is_decoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ca65a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_vector = multi_head.compute_attention(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "785db23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Query,Key,Value layer is ----> Linear(in_features=1024, out_features=1536, bias=True) \n",
      "\n",
      "The Query vector is ----> torch.Size([30, 8, 5, 64]) \n",
      "\n",
      "The key vector is ----> torch.Size([30, 8, 5, 64]) \n",
      "\n",
      "The value vector is ----> torch.Size([30, 8, 5, 64]) \n",
      "\n",
      "The multihead output vector is ----> torch.Size([30, 5, 512]) \n",
      "\n",
      "Multi-head Attention is at decoder ----> False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multi_head.get_attention_details(output_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
